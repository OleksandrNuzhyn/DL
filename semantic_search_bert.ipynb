{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': ['Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.'],\n",
       " 'passages': {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  'passage_text': [\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       "   \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       "   'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ',\n",
       "   'The inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is made of some type of resistance wire, normally Kanthal or nichrome. When a current is applied to the coil (resistance wire), it heats up and the heated coil then vaporizes the eliquid. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.',\n",
       "   'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;',\n",
       "   'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.',\n",
       "   'RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The “end” or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;',\n",
       "   'vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the user is.',\n",
       "   'A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.',\n",
       "   'Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. '],\n",
       "  'url': ['https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia',\n",
       "   'https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia',\n",
       "   'http://acronyms.thefreedictionary.com/RBA',\n",
       "   'https://www.slimvapepen.com/rebuildable-atomizer-rba/',\n",
       "   'http://rba-africa.com/about/what-is-rba/',\n",
       "   'http://resultsleadership.org/what-is-results-based-accountability-rba/',\n",
       "   'http://rba-africa.com/about/what-is-rba/',\n",
       "   'http://searchsecurity.techtarget.com/definition/risk-based-authentication-RBA',\n",
       "   'https://www.slimvapepen.com/rebuildable-atomizer-rba/',\n",
       "   'http://www.rbaconsulting.com/']},\n",
       " 'query': 'what is rba',\n",
       " 'query_id': 19699,\n",
       " 'query_type': 'description',\n",
       " 'wellFormedAnswers': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(examples):\n",
    "    queries = []\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    \n",
    "    for query, passages_dict in zip(examples['query'], examples['passages']):\n",
    "        positive = None\n",
    "        negative_list = []\n",
    "        \n",
    "        for passage, label in zip(passages_dict['passage_text'], passages_dict['is_selected']):\n",
    "            if label == 1 and positive is None:\n",
    "                positive = passage\n",
    "            elif label == 0:\n",
    "                negative_list.append(passage)\n",
    "        \n",
    "        if positive:\n",
    "            queries.append(query)\n",
    "            positives.append(positive)\n",
    "            negatives.append(negative_list[:5])\n",
    "    \n",
    "    return {\n",
    "        'queries': queries,\n",
    "        'positives': positives,\n",
    "        'negatives': negatives\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed = dataset.map(prepare_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queries': 'what is rba',\n",
       " 'positives': 'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.',\n",
       " 'negatives': [\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       "  \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       "  'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ',\n",
       "  'The inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is made of some type of resistance wire, normally Kanthal or nichrome. When a current is applied to the coil (resistance wire), it heats up and the heated coil then vaporizes the eliquid. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.',\n",
       "  'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS_LEN = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    query_input_ids = []\n",
    "    query_attention_masks = []\n",
    "    positives_input_ids = []\n",
    "    positives_attention_masks = []\n",
    "    negatives_input_ids = []\n",
    "    negatives_attention_masks = []\n",
    "\n",
    "    for i in range(len(examples[\"queries\"])):\n",
    "        query_tokens = tokenizer(\n",
    "            examples[\"queries\"][i],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_TOKENS_LEN,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        query_input_ids.append(query_tokens[\"input_ids\"][0])\n",
    "        query_attention_masks.append(query_tokens[\"attention_mask\"][0])\n",
    "\n",
    "        positive_tokens = tokenizer(\n",
    "            examples[\"positives\"][i],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_TOKENS_LEN,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        positives_input_ids.append(positive_tokens[\"input_ids\"][0])\n",
    "        positives_attention_masks.append(positive_tokens[\"attention_mask\"][0])\n",
    "\n",
    "        neg_input_ids = []\n",
    "        neg_attention_masks = []\n",
    "\n",
    "        for neg_text in examples[\"negatives\"][i]:\n",
    "            negative_tokens = tokenizer(\n",
    "                neg_text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=MAX_TOKENS_LEN,\n",
    "                return_tensors=\"tf\"\n",
    "            )\n",
    "            neg_input_ids.append(negative_tokens[\"input_ids\"][0])\n",
    "            neg_attention_masks.append(negative_tokens[\"attention_mask\"][0])\n",
    "        \n",
    "        negatives_input_ids.append(neg_input_ids)\n",
    "        negatives_attention_masks.append(neg_attention_masks)\n",
    "\n",
    "    return {\n",
    "        \"query_input_ids\": tf.stack(query_input_ids),\n",
    "        \"query_attention_mask\": tf.stack(query_attention_masks),\n",
    "        \"positives_input_ids\": tf.stack(positives_input_ids),\n",
    "        \"positives_attention_mask\": tf.stack(positives_attention_masks),\n",
    "        \"negatives_input_ids\": negatives_input_ids,\n",
    "        \"negatives_attention_mask\": negatives_attention_masks\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset_processed.map(tokenize_function, batched=True, remove_columns=dataset_processed['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "NUM_NEGATIVES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator():\n",
    "    for item in tokenized_dataset[\"train\"]:\n",
    "        if len(item[\"negatives_input_ids\"]) < NUM_NEGATIVES:\n",
    "            continue\n",
    "            \n",
    "        query_ids = np.array(item[\"query_input_ids\"], dtype=np.int32)\n",
    "        query_mask = np.array(item[\"query_attention_mask\"], dtype=np.int32)\n",
    "\n",
    "        pos_ids = np.array(item[\"positives_input_ids\"], dtype=np.int32)\n",
    "        pos_mask = np.array(item[\"positives_attention_mask\"], dtype=np.int32)\n",
    "        \n",
    "        neg_ids = np.array(item[\"negatives_input_ids\"][:NUM_NEGATIVES], dtype=np.int32)\n",
    "        neg_mask = np.array(item[\"negatives_attention_mask\"][:NUM_NEGATIVES], dtype=np.int32)\n",
    "        \n",
    "        yield (query_ids, query_mask, pos_ids, pos_mask, neg_ids, neg_mask)\n",
    "\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(224,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(224,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(224,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(224,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(5, 224), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(5, 224), dtype=tf.int32)\n",
    ")\n",
    "\n",
    "tf_train_dataset = tf.data.Dataset.from_generator(\n",
    "    dataset_generator,\n",
    "    output_signature=output_signature\n",
    ").shuffle(1000).batch(BATCH_SIZE).map(\n",
    "    lambda q_ids, q_mask, p_ids, p_mask, n_ids, n_mask: (\n",
    "        {\"input_ids\": q_ids, \"attention_mask\": q_mask},\n",
    "        {\"input_ids\": p_ids, \"attention_mask\": p_mask},\n",
    "        {\"input_ids\": n_ids, \"attention_mask\": n_mask}\n",
    "    )\n",
    ").cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator():\n",
    "    for item in tokenized_dataset[\"validation\"]:\n",
    "        if len(item[\"negatives_input_ids\"]) < NUM_NEGATIVES:\n",
    "            continue\n",
    "            \n",
    "        query_ids = np.array(item[\"query_input_ids\"], dtype=np.int32)\n",
    "        query_mask = np.array(item[\"query_attention_mask\"], dtype=np.int32)\n",
    "\n",
    "        pos_ids = np.array(item[\"positives_input_ids\"], dtype=np.int32)\n",
    "        pos_mask = np.array(item[\"positives_attention_mask\"], dtype=np.int32)\n",
    "        \n",
    "        neg_ids = np.array(item[\"negatives_input_ids\"][:NUM_NEGATIVES], dtype=np.int32)\n",
    "        neg_mask = np.array(item[\"negatives_attention_mask\"][:NUM_NEGATIVES], dtype=np.int32)\n",
    "        \n",
    "        yield (query_ids, query_mask, pos_ids, pos_mask, neg_ids, neg_mask)\n",
    "\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(224,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(224,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(224,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(224,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(5, 224), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(5, 224), dtype=tf.int32)\n",
    ")\n",
    "\n",
    "tf_valid_dataset = tf.data.Dataset.from_generator(\n",
    "    dataset_generator,\n",
    "    output_signature=output_signature\n",
    ").batch(BATCH_SIZE).map(\n",
    "    lambda q_ids, q_mask, p_ids, p_mask, n_ids, n_mask: (\n",
    "        {\"input_ids\": q_ids, \"attention_mask\": q_mask},\n",
    "        {\"input_ids\": p_ids, \"attention_mask\": p_mask},\n",
    "        {\"input_ids\": n_ids, \"attention_mask\": n_mask}\n",
    "    )\n",
    ").cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 2490\n",
      "Validation steps: 303\n"
     ]
    }
   ],
   "source": [
    "train_size = len(tokenized_dataset[\"train\"])\n",
    "valid_size = len(tokenized_dataset[\"validation\"])\n",
    "\n",
    "steps_per_epoch = train_size // BATCH_SIZE\n",
    "validation_steps = valid_size // BATCH_SIZE\n",
    "\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package=\"MyModels\")\n",
    "class BertWrapper(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model_name = model_name\n",
    "        self.model = TFBertModel.from_pretrained(model_name)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        return self.model(inputs, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"model_name\": self.model_name\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.saving.register_keras_serializable(package=\"MyModels\")\n",
    "class BiEncoder(tf.keras.Model):\n",
    "    def __init__(self, bert_model, projection_size=256, embedding_size=128, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.bert = bert_model\n",
    "        self.bert.trainable = False\n",
    "        self.projection_size = projection_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.projection = tf.keras.layers.Dense(projection_size, activation='gelu', kernel_regularizer=tf.keras.regularizers.l2(1e-5))\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.output_layer = tf.keras.layers.Dense(embedding_size)\n",
    "\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.mrr_tracker = tf.keras.metrics.Mean(name=\"mrr\")\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        is_3d = len(input_ids.shape) == 3\n",
    "        original_shape = tf.shape(input_ids)\n",
    "        \n",
    "        if is_3d:\n",
    "            input_ids = tf.reshape(input_ids, [-1, original_shape[-1]])\n",
    "            attention_mask = tf.reshape(attention_mask, [-1, original_shape[-1]])\n",
    "        \n",
    "        bert_outputs = self.bert({\"input_ids\": input_ids, \"attention_mask\": attention_mask}, training=False)\n",
    "\n",
    "        x = self.projection(bert_outputs.pooler_output, training=training)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        embeddings = self.output_layer(x, training=training)\n",
    "        embeddings = tf.nn.l2_normalize(embeddings, axis=1)\n",
    "    \n",
    "        if is_3d:\n",
    "            embeddings = tf.reshape(embeddings, [original_shape[0], original_shape[1], self.embedding_size])\n",
    "                                    \n",
    "        return embeddings\n",
    "  \n",
    "    # Multiple Negatives Ranking Loss\n",
    "    def compute_loss(self, query_emb, pos_emb, neg_emb):\n",
    "        batch_size = tf.shape(query_emb)[0]\n",
    "\n",
    "        pos_sim = tf.reduce_sum(query_emb * pos_emb, axis=1)\n",
    "        neg_sim = tf.reduce_sum(query_emb[:, None, :] * neg_emb, axis=-1)\n",
    "\n",
    "        similarities = tf.concat([pos_sim[:, None], neg_sim], axis=1)\n",
    "        labels = tf.zeros(batch_size, dtype=tf.int32)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(labels, similarities, from_logits=True)\n",
    "        \n",
    "        return loss, similarities\n",
    "    \n",
    "    # MRR для оцінки ранжування\n",
    "    def compute_mrr(self, similarities):\n",
    "        batch_size = tf.shape(similarities)[0]\n",
    "        labels = tf.zeros(batch_size, dtype=tf.int32)\n",
    "        \n",
    "        ranks = tf.argsort(similarities, axis=-1, direction='DESCENDING')\n",
    "        positive_ranks = tf.where(tf.equal(ranks, labels[:, None]))[:, 1]\n",
    "        reciprocal_ranks = 1.0 / (tf.cast(positive_ranks, tf.float32) + 1.0)\n",
    "        \n",
    "        return tf.reduce_mean(reciprocal_ranks)\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        query_inputs, positive_inputs, negative_inputs = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            query_embeddings = self(query_inputs, training=True)\n",
    "            positive_embeddings = self(positive_inputs, training=True)\n",
    "            negative_embeddings = self(negative_inputs, training=True)\n",
    "            loss, similarities = self.compute_loss(query_embeddings, positive_embeddings, negative_embeddings)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        mrr = self.compute_mrr(similarities)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.mrr_tracker.update_state(mrr)\n",
    "        \n",
    "        return {\"loss\": self.loss_tracker.result(), \"mrr\": self.mrr_tracker.result()}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        query_inputs, positive_inputs, negative_inputs = data\n",
    "        \n",
    "        query_embeddings = self(query_inputs, training=False)\n",
    "        positive_embeddings = self(positive_inputs, training=False)\n",
    "        negative_embeddings = self(negative_inputs, training=False)\n",
    "        loss, similarities = self.compute_loss(query_embeddings, positive_embeddings, negative_embeddings)\n",
    "        \n",
    "        mrr = self.compute_mrr(similarities)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.mrr_tracker.update_state(mrr)\n",
    "        \n",
    "        return {\"loss\": self.loss_tracker.result(), \"mrr\": self.mrr_tracker.result()}\n",
    "    \n",
    "    def get_text_embedding(self, inputs):\n",
    "        return self(inputs, training=False)\n",
    "        \n",
    "    # Цей метод дозволяє автоматично виводити метрики при навчанні через model.fit()\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.mrr_tracker]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"bert_model\": tf.keras.saving.serialize_keras_object(self.bert),\n",
    "            \"projection_size\": self.projection_size,\n",
    "            \"embedding_size\": self.embedding_size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        bert_model = tf.keras.saving.deserialize_keras_object(config[\"bert_model\"])\n",
    "        return cls(\n",
    "            bert_model=bert_model,\n",
    "            projection_size=config[\"projection_size\"],\n",
    "            embedding_size=config[\"embedding_size\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.saving.register_keras_serializable(package=\"MySchedules\")\n",
    "class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr=1e-3, decay_rate=0.9, decay_steps=2000, min_lr=1e-5):\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.min_lr = min_lr\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        decay_factor = self.decay_rate ** (step / self.decay_steps)\n",
    "        lr = self.initial_lr * decay_factor\n",
    "        lr = tf.maximum(lr, self.min_lr)\n",
    "                \n",
    "        return lr\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initial_lr\": self.initial_lr,\n",
    "            \"decay_rate\": self.decay_rate,\n",
    "            \"decay_steps\": self.decay_steps,\n",
    "            \"min_lr\": self.min_lr\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_wrapper = BertWrapper(\"bert-base-uncased\")\n",
    "bi_encoder = BiEncoder(bert_model=bert_wrapper)\n",
    "lr_schedule = CustomLearningRateSchedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_encoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), weighted_metrics=[\"mrr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2330/2490 [===========================>..] - ETA: 3:30 - loss: 1.7612 - mrr: 0.4681WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2490 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 303 batches). You may need to use the repeat() function when building your dataset.\n",
      "2490/2490 [==============================] - 3461s 1s/step - loss: 1.7612 - mrr: 0.4681 - val_loss: 1.7548 - val_mrr: 0.4730\n"
     ]
    }
   ],
   "source": [
    "history = bi_encoder.fit(tf_train_dataset, validation_data=tf_valid_dataset, epochs=EPOCHS, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(name=\"passages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(passages_batch, query_id, start_idx):\n",
    "    tokenized_batch = tokenizer(\n",
    "        passages_batch,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_TOKENS_LEN,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    embeddings_batch = bi_encoder({\n",
    "        \"input_ids\": tokenized_batch[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_batch[\"attention_mask\"]\n",
    "    }, training=False)\n",
    "\n",
    "    results = []\n",
    "    for idx, passage_text in enumerate(passages_batch):\n",
    "        results.append({\n",
    "            \"id\": f\"{query_id}_{start_idx + idx}\",\n",
    "            \"text\": passage_text,\n",
    "            \"embedding\": np.squeeze(embeddings_batch[idx].numpy()).tolist()\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def process_and_store(dataset, batch_size=32):\n",
    "    with tqdm(total=len(dataset[\"test\"]), desc=\"Обробка запитів\") as pbar:\n",
    "        for item in dataset[\"test\"]:\n",
    "            passages = item[\"passages\"][\"passage_text\"]\n",
    "            query_id = item[\"query_id\"]\n",
    "            \n",
    "            for start_idx in range(0, len(passages), batch_size):\n",
    "                passages_batch = passages[start_idx:start_idx + batch_size]\n",
    "                \n",
    "                results_batch = process_batch(passages_batch, query_id, start_idx)\n",
    "                \n",
    "                for result in results_batch:\n",
    "                    collection.add(\n",
    "                        ids=[result[\"id\"]],\n",
    "                        documents=[result[\"text\"]],\n",
    "                        embeddings=[result[\"embedding\"]]\n",
    "                    )\n",
    "            \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обробка запитів: 100%|██████████| 9650/9650 [24:54<00:00,  6.46it/s]\n"
     ]
    }
   ],
   "source": [
    "process_and_store(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query = \"does human hair stop squirrels\"\n",
      "   one acre equals 0 0015625 square miles 4840 square yards 43560 square feet or about 4047 square metres 0 405 hectares see below\n",
      "   Karen Nicol is an embroidery and mixed media textile artist working in gallery, fashion and interiors with a London based design and production studio established for over twenty-five years. \n",
      "   Trap rock is a name used in the construction industry for any dark-colored igneous rock that is used to produce crushed stone.\n",
      "   Pilocarpine is a drug used to treat dry mouth and glaucoma. It is a parasympathomimetic alkaloid obtained from the leaves of tropical South American shrubs from the genus Pilocarpus. Pilocarpine is used to stimulate sweat glands in a sweat test to measure the concentration of chloride and sodium that is excreted in sweat. It is used to diagnose cystic fibrosis.\n",
      "   A castle (from Latin: castellum) is a type of fortified structure built in Europe and the Middle East during the Middle Ages by nobility. Scholars debate the scope of the word castle, but usually consider it to be the private fortified residence of a lord or noble. \n",
      "\n",
      "query = \"what are the benefits of fossil fuels\"\n",
      "   according to football finance experts at deloitte the average wage for premier league players rose to £ 1 6m during the 2012 13 season the latest available data that equates to £ 31000 a week which is more than the average uk worker earns in a year special report a richer world \n",
      "   Origin of DETERIORATE. Late Latin deterioratus, past participle of deteriorare, from Latin deterior worse, from de- + -ter (suffix as in Latin uter which of two) + -ior (comparative suffix) — more at whether, -er. First Known Use: 1572.\n",
      "   report abuse the french wore blue uniforms with white trim the british wore red with white trim the native americans wore camouflage with war paint rating newest oldest best answer british wore red mostly french wore white mostly there were other colors though green rogers rangers blue american continentals\n",
      "   volcano is located close to the center of california volcano is part of amador county volcano has 1 50 square miles of land area and has no water area as of 2010 the total volcano population is 115 volcano median household income is $ 89632 in 2008 2012\n",
      "   dutch sint-maarten. In 1954, the Netherlands granted the Netherlands Antilles (Curacao, Curaçao, Aruba, Bonaire, Saba statia and The dutch part Of. St) martin a status giving them internal autonomy within The Dutch kingdom on a large. scale\n",
      "\n",
      "query = \"what is a apothem\"\n",
      "   Paprika is a 1991 Italian film directed by Tinto Brass. The film is loosely based on John Cleland's novel Fanny Hill, first published in 1748. It was remade as an explicit pornographic film by Joe D'Amato in 1995.\n",
      "   Paprika is a 1991 Italian film directed by Tinto Brass. The film is loosely based on John Cleland's novel Fanny Hill, first published in 1748. It was remade as an explicit pornographic film by Joe D'Amato in 1995.\n",
      "   The Schieffen Plan was Germany's key war plan for invading France at the start of World War 1. It involved marching through Belgium in order to knock France out of the war quickly. The plan was devised by Alfred von Schlieffen. It involved attacking France through Belgium in the event of war with France. The Schlieffen Plan (in World War I) was named for its author, the German army chief of staff Count Alfred von Schlieffen, who formulated it in 1905.\n",
      "   Karen Nicol is an embroidery and mixed media textile artist working in gallery, fashion and interiors with a London based design and production studio established for over twenty-five years. \n",
      "   Constantin Stanislavski was a Russian stage actor and director who developed the naturalistic performance technique known as the Stanislavsky method, or method acting.\n",
      "\n",
      "query = \"average cost for custom canopy\"\n",
      "   Magma is highly heated molten rock that comes to the surface of  usually through volcanic eruption. Magma is usually found beneath  the earth located near the earth's core a … nd can sometimes contain  crystals and gases. Answered. \n",
      "   In Basketball Rules and Regulations. The dimensions of an indoor soccer field are 200 feet long by 85  feet wide. The games are divided into four quarters of 15 minutes  each for a total of 60 minutes of play t … ime. Answered.\n",
      "   Marzipan is a candy paste made primarily from almonds and sugar. It is used as a candy filling, and is a common ingredient in many European pastries and cakes. Because it has a smooth, supple texture, marzipan is also used to shape and mold candy figures, and can be used as an alternative to fondant in covering cakes. \n",
      "   Homie (from  homeboy ) is an English language slang term found in American urban culture, whose origins etymologists generally trace to Mexican American Spanglish from the late 19th century, with the word homeboy meaning a male friend from back home. \n",
      "   Arnuity Ellipta (fluticasone furoate) Inhalation Powder is a corticosteroid used for the once-daily maintenance treatment of asthma as prophylactic therapy in patients aged 12 years and older. Common side effects include bronchitis, headache, cold symptoms, upper respiratory tract infection, sore throat, and sinusitis. \n",
      "\n",
      "query = \"what is a hardware in a computer\"\n",
      "   one acre equals 0 0015625 square miles 4840 square yards 43560 square feet or about 4047 square metres 0 405 hectares see below\n",
      "   Trap rock is a name used in the construction industry for any dark-colored igneous rock that is used to produce crushed stone.\n",
      "   Nylatron is a tradename for a family of nylon plastics, typically filled with molybdenum disulfide lubricant powder. It is used to cast plastic parts for machines, because of its mechanical properties and wear-resistance.\n",
      "   The 1944 battle of Leyte Gulf was the battle to take the Gulf of Leyte. It was part of the Philippines campaign. During the battle, over 100 Japanese Kamikazes, filled with fu … el and explosive, were used. They had to crash into ships in order to deal damage to them. The Battle of Leyte Gulf involved the Japanese forces successfully  diverting the US forces away from the Leyte island. The Battle of  Leyte occurred in October 1944.\n",
      "   Pilocarpine is a drug used to treat dry mouth and glaucoma. It is a parasympathomimetic alkaloid obtained from the leaves of tropical South American shrubs from the genus Pilocarpus. Pilocarpine is used to stimulate sweat glands in a sweat test to measure the concentration of chloride and sodium that is excreted in sweat. It is used to diagnose cystic fibrosis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def select_top_5_queries_with_selected(dataset):\n",
    "    selected_queries = []\n",
    "    for item in dataset[\"test\"]:\n",
    "        if 1 in item[\"passages\"][\"is_selected\"]:\n",
    "            selected_queries.append(item)\n",
    "        if len(selected_queries) >= 5:\n",
    "            break\n",
    "    return selected_queries\n",
    "\n",
    "def search(selected_queries, n_results=5):\n",
    "    for item in selected_queries:\n",
    "        tokenized_query = tokenizer(\n",
    "            item[\"query\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_TOKENS_LEN,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "\n",
    "        query_embedding = bi_encoder({\n",
    "            \"input_ids\": tokenized_query[\"input_ids\"],\n",
    "            \"attention_mask\": tokenized_query[\"attention_mask\"]\n",
    "        }, training=False).numpy()[0].tolist()\n",
    "\n",
    "        search_results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results\n",
    "        )\n",
    "        print(f'query = \"{item[\"query\"]}\"')\n",
    "        [print(\"   \" + str(result)) for result in np.squeeze(search_results[\"documents\"])]\n",
    "        print()\n",
    "\n",
    "selected_queries = select_top_5_queries_with_selected(dataset)\n",
    "search(selected_queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
